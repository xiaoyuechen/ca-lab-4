Part 1:

  Q1: The hit rate is 0%. The cache block size is 4 bytes, so we could only pull
  the current element we are accessing to the cache. When we access the same
  element after we have iterated over all the 255 other elements, it has already
  been evicted from the cache by other elements because we only have 512 bytes
  in cache in total, which is only enough for 126 elements.

  Q2: since we miss each cache access see (Q1), we have a total of (number of elements)*(number of itterations)=256*10=2560 cache misses. 
  Compulsory misses are misses that can't be avoided by a larger cache or from larger associativity sets. 
  Since the block size is the same as the element size, and we at least will get each element into the cache.
  We get a total of one compulsory miss for each element = 256
  
  Q3: 
  Compulsory misses: Here we store two elements in each block and so we only
  "need" (nr of elements) / (elements per block) = (256)/(2) = 128 cache accesses
  to have accessed all of the data at least once. So we will have 128 compulsory misses.
  
  Q4: The hit rate does not change. We take the last bit of memory address
  to decide which set of cache the word should go to. When we access the same
  element after we have iterated over all the 255 other elements, it has already
  been evicted from the cache by the element whose memory address is the memory
  address of the element we are accessing + 8. So the hit rate would still be 0%.

  Q5: Since each block is as large as an element,we only load one element into the 
  cache for each miss so we don't g5et any advantage from data locality within a single matrix iteration. 
  This is true for both cache types.
  The second opportunity to get hits from data locality is after the first iteration through the matrix, 
  but ONLY IF the cache can store the whole matrix. Meaning we need at least 
  a cache size of (size of element)(nr of elements) = 4 * 256 = 1024 bytes

  Q6: Block size of 16 bytes gives the best hit rate 98%. Since we have a cache
  with total size of 1024 bytes, we never evict any elements from the cache.
  Therefore, the more data we could fetch to the cache each time, the better hit
  rate we would have. 16 bytes block would fetch 4 elements at a time. It is the
  best for the hit rate.

  Q7: Since the cache size is big enough for all the elements, we won't have
  cache misses after the first iteration. During the first iteration, we would
  miss 64 times. Total memory access count of the program is 64 * 4 * 10. 
  So the hit rate is 1 - (64/(64*4*10)) = 97.5%.
  
Part 2:

  Q1: The hit rate does not increase (0%), The fundamental problem is similar to before.
  The block can only hold one element so we won't get any "data locality benefits" from 
  within the loop. Furthermore the size of the matrix is larger than the size of the cache,
  or in other words, there are more elements in the matrix than there are blocks in the cache. 
  This means that the data stored in cache from the first matrix iteration will have been evicted
  before the program needs it at the second iteration etc. computation: 0 hits/2560 accesses = 0%
  
  Q2: The hit rate improve to 50%. This is because we pull the element and the
  element after it into the 8-byte block every time. Therefore 1/2 = 50%.

  Q3: The hit rate improve to 75%. Similar to Q2, we pull 4 element into the
  cache each time, hence 3/4 = 75% hit rate.

  Q4: We miss all access when pulling during the 1st of the 10 iterations. So
  9/10 = 90% hit rate.

  Q5: We only miss during the 1st of the 10 iterations. During the 1st
  iteration, we miss 50% of the time. So overall we miss 0.5 * 1 / 10 = 5%. 
  Hence hit rate 95%.
  
  Q6: We will miss during the first iteration of the matrix every element in the 
  first column as a whole row "fits" in the block. So a total of (nr of rows) = 
  = 64 misses. miss rate = 64/2560 = 2.5%  --> 97.5% hit rate
  
  Q7: No. We need to load data at least once from the memory to cache. This
  load is unavoidable no matter what cache scheme we use. With the best cache
  scheme, we could approach to 100% hit rate but we could never reach it.

Bonus:

  Q1: The hit rate is 0%. This is because array A and B conflict with each other
  on their index bits. Loading an element in B would evict the cached element
  in A with the same index. This is because we have 128 blocks in cache which
  requires 7 bits for the index. If we take the last 7 bits of memory addresses
  as indices, when 2 memory addresses are 128 words apart, they would conflict
  with each other and result in eviction of the first element. It is exactly
  what happens in this case since array A and B are 128 words apart. So every
  load on B's element would cause a conflict miss. Therefore, the hit rate is
  0%.

  Q2: Changing the block size does not change the hit rate. It remains 0%. The
  reason is that conflict miss still happens on each memory access. Element in
  B will still evict element in A every time.

  Q3: We do not have any conflict misses in this case. With 2-way set
  associative, we use the last bit of memory address to decide which set the
  element belongs to. As there are 64 elements in A and B with last memory
  address bit 0 and other 64 elements with last memory address bit 1, no
  eviction would happen. All 128 elements fit in the cache perfectly.

  Therefore, We only have compulsory misses. The numbers of compulsory misses
  are 128. So we have overall miss rate 128/(10*128) = 10%. Hence hit rate
  is 90%.
    
  Q4: The hit rate is the same at a 90%. Since the block size is the same one can
  draw the conclussion that these 10% are purely compulsory misses, same as Q3.
  Why? In 4-way associativity the last two bits determine which set a memory 
  address is "assigned" to, basicaly modulo_4. Here 1/4 of the array (16 words) 
  will be assigned into one set, and the same for the other three quarters.
  This will evenly fill up all four sets, and so no conflict misses will occur.
  Only the 128 compulsory misses are left.

  Q5: Increasing the associativity of a cache would decrease the conflict
  misses, hence increasing the overall hit rate. However, it would also result
  in increased financial and power cost of the cache. 
